{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An MCMC approach to the outburst decay model \n",
    "Based on [Powell et al. 2007](https://ui.adsabs.harvard.edu/abs/2007MNRAS.374..466P/abstract) and [Heinke et al. 2015](https://ui.adsabs.harvard.edu/abs/2015MNRAS.447.3034H/abstract).  We deploy this via [PyMC 3](https://docs.pymc.io/) ([Salvatier et al. 2016](https://doi.org/10.7717/peerj-cs.55)) package and use HMC ([Neal 2012](https://ui.adsabs.harvard.edu/?#abs/2012arXiv1206.1901N), [Betancourt 2017](https://ui.adsabs.harvard.edu/?#abs/2017arXiv170102434B)) with NUTS sampling ([Hoffman & Gelman 2011](https://ui.adsabs.harvard.edu/?#abs/2011arXiv1111.4246H)). It can be overkill in most cases, but given the model is piece-wise and data can be crummy, this bruteforce approach allows overcoming many of these issues.\n",
    "\n",
    "## Setting up and loading necessary packages\n",
    "**Packages used:**\n",
    "- [Astropy](http://www.astropy.org/) ([Robitaille et al. 2013](https://ui.adsabs.harvard.edu/?#abs/2013A&A...558A..33A))\n",
    "- [Corner](https://corner.readthedocs.io/en/latest/) ([Foreman-Mackey 2016](http://joss.theoj.org/papers/10.21105/joss.00024))\n",
    "- [Matplotlib](https://matplotlib.org/) ([Hunter 2007](https://ieeexplore.ieee.org/document/4160265))\n",
    "- [Numpy](http://www.numpy.org/) ([Oliphant 2006](https://archive.org/details/NumPyBook/page/n0))\n",
    "- [PyMC 3](https://docs.pymc.io/) ([Salvatier et al. 2016](https://doi.org/10.7717/peerj-cs.55))\n",
    "- [Theano](http://deeplearning.net/software/theano/) ([Al-Rfou et al. 2016](https://ui.adsabs.harvard.edu/abs/2016arXiv160502688T/abstract); used through PyMC3, no need to install separately)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed: 730093409\n",
      "Running on:\n",
      "\tPython:\t\t3.7.2\n",
      "\tAstropy\t\t4.0.1.post1\n",
      "\tCorner\t\t2.2.1\n",
      "\tMatplotlib\t3.2.1\n",
      "\tNumpy\t\t1.18.4\n",
      "\tPyMC\t\t3.11.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import corner\n",
    "import astropy\n",
    "from astropy.io import ascii\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "rc('text', usetex=False)\n",
    "rc('font', family='serif')\n",
    "\n",
    "# rand_seed = np.random.randint(0, 2**32 - 1)\n",
    "rand_seed = np.random.randint(0, 2**31 - 1)\n",
    "print('Random seed:',rand_seed)\n",
    "\n",
    "# Package versions:\n",
    "print('Running on:')\n",
    "print(f'\\tPython:\\t\\t{sys.version[:5]}')\n",
    "print('\\tAstropy\\t\\t{}'.format(astropy.__version__))\n",
    "print('\\tCorner\\t\\t{}'.format(corner.__version__))\n",
    "print('\\tMatplotlib\\t{}'.format(plt.matplotlib.__version__))\n",
    "print('\\tNumpy\\t\\t{}'.format(np.__version__))\n",
    "print('\\tPyMC\\t\\t{}'.format(pm.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing data\n",
    "\n",
    "Typically the light curves contain data points from before or after the interval our model can reliably model. there are two statistical solutions to this: 1- either to define a parameter in the bayesian modeling framework that can probabilistically determine the optimized interval, or 2- do it manually based on visual inspection. \n",
    "\n",
    "While the former may be more appealing, it requires lots of hand-holding for modeling and care, and needs relatively good data (high number of data points), and it can be very slow. So, for many cases, the latter is far easier with mostly similar results. Here we use the latter, but I have developed a framework for the former method as well if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ascii.read('data/decayfits/xtej1728295_unabsorbedflux.txt', data_start=3, data_end=37)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering and preparing the data could be an iterative process and depends on the data in each case, here's an example were I grab the columns from the table above and cut out data outside my range of interest.\n",
    "\n",
    "(In this particular case, there aren't any data points outside the range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outburst_start = 58583\n",
    "outburst_end = 58712\n",
    "\n",
    "# Columns names\n",
    "obsdate = 'Obsdate'\n",
    "flux = 'flux'\n",
    "flux_ler = 'flux_ler'\n",
    "\n",
    "\n",
    "# Filtering to only include the data in the interval above\n",
    "t_obs = data[obsdate].data[(data[obsdate] >= outburst_start) & (data[obsdate] <= outburst_end)]\n",
    "L_obs = data[flux].data[(data[obsdate] >= outburst_start) & (data[obsdate] <= outburst_end)]\n",
    "L_obs_err = data[flux_ler].data[(data[obsdate] >= outburst_start) & (data[obsdate] <= outburst_end)]\n",
    "\n",
    "# We will keep the rest for plotting purposes\n",
    "t_ignor = data[obsdate].data[(data[obsdate] < outburst_start) | (data[obsdate] > outburst_end)]\n",
    "L_ignor = data[flux].data[(data[obsdate] < outburst_start) | (data[obsdate] > outburst_end)]\n",
    "L_ignor_err = data[flux_ler].data[(data[obsdate] < outburst_start) | (data[obsdate] > outburst_end)]\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(t_obs,L_obs,L_obs_err,fmt='.b',elinewidth=0.5,capsize=0,alpha=1.0,ms=4.0)\n",
    "ax.errorbar(t_ignor,L_ignor,L_ignor_err,fmt='.k',elinewidth=0.5,capsize=0,alpha=0.2,ms=4.0)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Time (MJD)', fontsize=14)\n",
    "ax.set_ylabel('Luminosity or Flux or Count rate', fontsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='both', which='major', length=9)\n",
    "ax.tick_params(axis='both', which='minor', length=4.5)\n",
    "ax.tick_params(axis='both', which='both',direction='in',right=True,top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup and fitting preparation\n",
    "\n",
    "The light curve decay model (based on [Powell et al. 2007](https://ui.adsabs.harvard.edu/abs/2007MNRAS.374..466P/abstract) and [Heinke et al. 2015](https://ui.adsabs.harvard.edu/abs/2015MNRAS.447.3034H/abstract)):\n",
    "\n",
    "$$ L(t) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            (L_t - L_e) \\exp\\left( - \\frac{t-t_t}{\\tau_e} \\right)+L_e & \\quad t \\leq t_t \\\\\n",
    "            L_t \\left( 1-\\frac{t-t_t}{\\tau_l} \\right) & \\quad t > t_t\n",
    "        \\end{array}\n",
    "    \\right. $$\n",
    "\n",
    "Where:\n",
    "- $L_t$: Luminosity at the brink\n",
    "- $L_e$: Exponential asymptotic luminosity ($0.4 L_t \\leq L_e \\leq L_t $)\n",
    "- $t_t$: Time of the brink\n",
    "- $\\tau_e$: Exponential decay timescale\n",
    "- $\\tau_l$: Linear decay timescale\n",
    "\n",
    "**NOTE: Through out this notebook, I use the word \"luminosity\" a bit loosely, $L_t$,$L_e$,$L(t)$ will be the same type of quantity and in the same unit as your input data (e.g., Flux, count rate, etc).**\n",
    "\n",
    "Given the piece-wise nature of the model, setting it up for MCMC is delicate, so we have two versions it coded up, a \"simple\" model which is algebraic and straight-forward, and a second one labeled \"theano\", which is optimized for PyMC with Theano tensors. The difference is subtle and minor: it is only that the variables are expected to be  scalar quantities or `numpy` arrays for the \"simple\" model, while they are expected to be Theano tensors for a proper MCMC. Thus, the simple model is easy to use for plotting and checking, while the theano version is prepared for our MCMC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model:\n",
    "def decay_model_simple(t,Lt,Le,tt,tau_e,tau_l):\n",
    "    \"\"\"\n",
    "    Simple model to fit outburst decay of XRBs. \n",
    "    Consists of an initial exponential decay followed by a linear decay.\n",
    "    \n",
    "    This is a simple version.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: time (an array, values in MJD)\n",
    "    \n",
    "    Lt: luminosity at the brink\n",
    "    \n",
    "    Le: exponential asymptotic luminosity\n",
    "    \n",
    "    tt: time of the brink\n",
    "    \n",
    "    tau_e: exp decay timescale\n",
    "    \n",
    "    tau_l: linear decay timescale\n",
    "    \n",
    "    Output:\n",
    "    -----------\n",
    "    L: luminosity at time t\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    Lexp = (Lt-Le)*np.exp(-(t[t <= tt]-tt)/tau_e)+Le    \n",
    "    Llin = Lt*(1 - (t[t > tt]-tt)/tau_l)\n",
    "    L = np.concatenate((Lexp,Llin))\n",
    "    return L\n",
    "\n",
    "\n",
    "def decay_model_theano(t,Lt,Le,tt,tau_e,tau_l):\n",
    "    \"\"\"\n",
    "    Simple model to fit outburst decay of XRBs. \n",
    "    Consists of an initial exponential decay followed by a linear decay.\n",
    "    \n",
    "    This is a Theano-compatible version.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t: time (an array, values in MJD)\n",
    "    \n",
    "    Lt: luminosity at the brink\n",
    "    \n",
    "    Le: exponential asymptotic luminosity\n",
    "    \n",
    "    tt: time of the brink\n",
    "    \n",
    "    tau_e: exp decay timescale\n",
    "    \n",
    "    tau_l: linear decay timescale\n",
    "    \n",
    "    Output:\n",
    "    -----------\n",
    "    L: luminosity at time t\n",
    "    \n",
    "    \"\"\"    \n",
    "    L = pm.math.switch(t <= tt, (Lt-Le)*np.exp(-(t-tt)/tau_e)+Le, Lt*(1 - (t-tt)/tau_l))\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraining the parameter space to search\n",
    "\n",
    "We need to find initial estimates for the parameters in the model to start fitting (e.g., if we are off by a really large value in parameter, no matter how good the fitting algorithm, finding the best fit will be a challenge). So we iteratively test values until the model looks to roughly follow the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial guess values:\n",
    "test_Lt = 3e-11\n",
    "test_Le = 2e-11\n",
    "test_tt = 58640\n",
    "test_tau_e = 20\n",
    "test_tau_l = 100\n",
    "\n",
    "plot_t = np.linspace(t_obs.min(),t_obs.max(),num=2000)\n",
    "plot_l = decay_model_simple(plot_t,test_Lt,test_Le,test_tt,test_tau_e,test_tau_l)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(t_obs,L_obs,L_obs_err,fmt='.b',elinewidth=0.5,capsize=0,alpha=1.0,ms=4.0, label='Data')\n",
    "ax.plot(plot_t,plot_l,'-r', label='Model (initial guess)')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=14)\n",
    "ax.minorticks_on()\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "ax.tick_params(axis='both', which='major', length=9)\n",
    "ax.tick_params(axis='both', which='minor', length=4.5)\n",
    "ax.tick_params(axis='both', which='both',direction='in',right=True,top=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is close enough!\n",
    "\n",
    "## Setting up the priors and the bayesian inference\n",
    "\n",
    "Now, onto the actual fitting: we need to set up priors for our parameters and connect the model and data via likelihood. Our data consists of luminosity/flux/countrate measurements $y=[y_1,y_2,\\cdots,y_N]$ and measurement uncertainties of $\\sigma=[\\sigma_1,\\sigma_2,\\cdots,\\sigma_N]$ for times $t=[t_1,t_2,\\cdots,t_N]$ and as before, our model was:\n",
    "\n",
    "$$ L(t) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            (L_t - L_e) \\exp\\left( - \\frac{t-t_t}{\\tau_e} \\right)+L_e & \\quad t \\leq t_t \\\\\n",
    "            L_t \\left( 1-\\frac{t-t_t}{\\tau_l} \\right) & \\quad t > t_t\n",
    "        \\end{array}\n",
    "    \\right. $$\n",
    "\n",
    "Where:\n",
    "- $L_t$: Luminosity at the brink\n",
    "- $L_e$: Exponential asymptotic luminosity ($0.4 L_t \\leq L_e \\leq L_t $)\n",
    "- $t_t$: Time of the brink\n",
    "- $\\tau_e$: Exponential decay timescale\n",
    "- $\\tau_l$: Linear decay timescale\n",
    "\n",
    "So, we define our statistical model as:\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "p(L_t,L_e,t_t,\\tau_e,\\tau_l|y) &\\propto p(L_t,L_e,t_t,\\tau_e,\\tau_l)~p(y|L_t,L_e,t_t,\\tau_e,\\tau_l)\\\\\n",
    "p(L_t,L_e,t_t,\\tau_e,\\tau_l|y) &\\propto p(L_t,L_e,t_t,\\tau_e,\\tau_l)~\\mathcal{L}(y,L(t))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$$\n",
    "\\mathcal{L}(y,L(t)) = \\prod_{i=1}^N \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left[\\frac{-(y_i - L(t))^2}{2\\sigma_i^2} \\right]\n",
    "$$\n",
    "\n",
    "despite its intimidating appearance is a simple Gaussian likelihood, and we assume the following priors:\n",
    "- $\\log_{10}(L_t) \\sim \\mathcal{U}(\\log_{10}y_\\min,\\log_{10}y_\\max)$\n",
    "- $\\log_{10}(L_e) \\sim \\mathcal{U}(\\log_{10}(0.4 L_t),\\log_{10}(L_t))$\n",
    "- $t_t \\sim \\mathcal{U}(t_\\min, t_\\max)$\n",
    "- $\\tau_e \\sim \\mathcal{U}(?)$\n",
    "- $\\tau_l \\sim \\mathcal{U}(?)$\n",
    "\n",
    "Where $\\mathcal{U}(a,b)$ means a uniform prior probability distribution between $a$ and $b$. You notice that we defined the priors on the logarithm of $Lt$ and $Le$ instead of them. This is to consider [scale invariance](https://en.wikipedia.org/wiki/Scale_invariance) for these parameters. Simply put, this is because we want the prior probability of getting a value between $10^{-12}$ and $10^{-11}$ to be the same as $10^{-11}$ and $10^{-10}$. The priors for $\\tau_e$ and $\\tau_l$ depend on the case and need some consideration for every source separately.\n",
    "\n",
    "\n",
    "## Fitting the data\n",
    "\n",
    "While the notation and background for the concepts mentioned above may complex, implementing them for our fitting is actually rather simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    #Lt = pm.Uniform('Lt',1e-12,3e-10)\n",
    "    #Le = pm.Uniform('Le',0.4*Lt,Lt)\n",
    "    logLt = pm.Uniform('logLt',-12,-10)\n",
    "    logLe = pm.Uniform('logLe',np.log10(0.4)+logLt,logLt)\n",
    "    tt = pm.Uniform('tt',58580,58710)\n",
    "    tau_e = pm.Uniform(r'tau\\_e',1,200)\n",
    "    tau_l = pm.Uniform(r'tau\\_l',1,200)\n",
    "    Lum = pm.Normal('Luminosity',\n",
    "                    mu=decay_model_theano(t_obs,10**logLt,10**logLe,tt,tau_e,tau_l),\n",
    "                    sd=L_obs_err,observed=L_obs)\n",
    "#     trace1 = pm.sample(draws=20000, chains=6, cores=12, tune=10000, random_seed=rand_seed)\n",
    "    trace1 = pm.sample(draws=10, chains=6, cores=12, tune=10, random_seed=rand_seed)\n",
    "\n",
    "pm.summary(trace1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's go through the outputs above:\n",
    "\n",
    "\n",
    "```\n",
    "Auto-assigning NUTS sampler...\n",
    "Initializing NUTS using jitter+adapt_diag...\n",
    "Multiprocess sampling (6 chains in 12 jobs)\n",
    "NUTS: [tau\\_l, tau\\_e, tt, logLe, logLt]\n",
    "Sampling 6 chains, 2,607 divergences: 100%|██████████| 180000/180000 [01:12<00:00, 2487.84draws/s]\n",
    "```\n",
    "This bit is just telling us that it is setting it up and running it, and the last line is a simple progress bar.\n",
    "\n",
    "---\n",
    "```\n",
    "There were 73 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "There were 221 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "There were 1175 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "The acceptance probability does not match the target. It is 0.5502145415237404, but should be close to 0.8. Try to increase the number of tuning steps.\n",
    "There were 492 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "The acceptance probability does not match the target. It is 0.7201504152732185, but should be close to 0.8. Try to increase the number of tuning steps.\n",
    "There were 367 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "The acceptance probability does not match the target. It is 0.7084498025442265, but should be close to 0.8. Try to increase the number of tuning steps.\n",
    "There were 279 divergences after tuning. Increase `target_accept` or reparameterize.\n",
    "The number of effective samples is smaller than 10% for some parameters.\n",
    "```\n",
    "This bit is telling us that our MCMC has generated some values that are not as \"convergent\" as expected. These are mostly minor and fine for now and we can ignore them for the moment (later, when we are benchmark testing the results, we can address them by increasing the values of `draw` and `tune` in our sampling.\n",
    "\n",
    "## Visualizing the results\n",
    "\n",
    "We are going to make a \"trace\" plot which will demonstrate how our MCMC sampling went, and a \"corner\" plot, which demonstrates how the posterior samples (best-fit value and uncertainties) look for each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.plot_trace(trace1)\n",
    "#pm.plot_posterior(trace1,point_estimate='mode')\n",
    "\n",
    "\n",
    "# Corner plot with distribution modes:\n",
    "ndim = 5\n",
    "medians = [np.median(trace1['logLt']),\n",
    "           np.median(trace1['logLe']),\n",
    "           np.median(trace1['tt']),\n",
    "           np.median(trace1['tau\\_e']),\n",
    "           np.median(trace1['tau\\_l'])]\n",
    "\n",
    "corner_data = list(zip(*np.array([trace1.get_values('logLt'),\n",
    "                                  trace1.get_values('logLe'),\n",
    "                                  trace1.get_values('tt'),\n",
    "                                  trace1.get_values('tau\\_e'),\n",
    "                                  trace1.get_values('tau\\_l')])))\n",
    "\n",
    "cornplot = corner.corner(corner_data,show_titles=True,use_math_text=False,\n",
    "                         labels = [r'$\\logL_t$',r'$\\logL_e$',r'$t_t$',r'$\\tau_e$',r'$\\tau_l$'],\n",
    "                         title_kwargs={\"fontsize\": 14},label_kwargs={\"fontsize\": 14})\n",
    "\n",
    "axes = np.array(cornplot.axes).reshape((ndim, ndim))\n",
    "for i in range(ndim):\n",
    "    ax = axes[i, i]\n",
    "    ax.axvline(medians[i], color=\"r\")\n",
    "\n",
    "for yi in range(ndim):\n",
    "    for xi in range(yi):\n",
    "        ax = axes[yi, xi]\n",
    "        ax.axvline(medians[xi], color=\"r\")\n",
    "        ax.axhline(medians[yi], color=\"r\")\n",
    "        ax.plot(medians[xi], medians[yi], \"sr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_start_date = 58580  # mjd\n",
    "plot_length = 145             # days\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "\n",
    "medians = [10**np.median(trace1['logLt']),\n",
    "           10**np.median(trace1['logLe']),\n",
    "           np.median(trace1['tt']),\n",
    "           np.median(trace1['tau\\_e']),\n",
    "           np.median(trace1['tau\\_l'])]\n",
    "\n",
    "\n",
    "ax1 = plt.subplot2grid((4,1), (0,0), rowspan = 3,fig=fig)\n",
    "plot_t = np.linspace(plot_start_date,plot_start_date+plot_length,num=200)\n",
    "plot_L = decay_model_simple(plot_t, medians[0], medians[1], medians[2], medians[3], medians[4])\n",
    "\n",
    "\n",
    "for i in np.random.randint(0,len(trace1),size=200):\n",
    "    plot_L_samp = decay_model_simple(plot_t, 10**trace1['logLt'][i], 10**trace1['logLe'][i], \n",
    "                              trace1['tt'][i], trace1['tau\\_e'][i], trace1['tau\\_l'][i])\n",
    "    ax1.plot(plot_t-plot_start_date,plot_L_samp,'c',lw=0.5,alpha=0.05)\n",
    "    #plt.plot(trace1['tt'][i],decay_model_simple(trace1['tt'][i], \n",
    "    #                                            trace1['Lt'][i], \n",
    "    #                                            trace1['Le'][i], \n",
    "    #                                            trace1['tt'][i], \n",
    "    #                                            trace1['tau\\_e'][i], \n",
    "    #                                            trace1['tau\\_l'][i]),'+k',alpha=0.1)\n",
    "\n",
    "ax1.plot(plot_t-plot_start_date,plot_L,'b',lw=0.8)\n",
    "\n",
    "#plt.plot(medians[2],decay_model_simple(medians[2],medians[0], medians[1], medians[2], medians[3], medians[4]),'+k')\n",
    "\n",
    "ax1.errorbar(t_obs-plot_start_date,L_obs,L_obs_err,fmt='.r',elinewidth=0.8,capsize=0,ms=5.0,alpha=0.9)\n",
    "\n",
    "ax1.errorbar(t_ignor-plot_start_date,L_ignor,L_ignor_err,fmt='.r',elinewidth=0.8,capsize=0,ms=5.0,alpha=0.1)\n",
    "\n",
    "ax1.set_xlim(0,plot_length)\n",
    "ax1.set_ylim(9e-13,3e-10)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_ylabel(r'Flux (erg cm$^{-2}$ s$^{-1}$)',fontsize=14)\n",
    "ax1.set_xticklabels([])\n",
    "\n",
    "ax1.minorticks_on()\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax1.tick_params(axis='both', which='major', length=5)\n",
    "ax1.tick_params(axis='both', which='minor', length=2.5)\n",
    "ax1.tick_params(axis='both', which='both',direction='in',right=True,top=True)\n",
    "\n",
    "ax2 = plt.subplot2grid((4,1), (3,0), fig=fig)\n",
    "sig = np.ones(len(t_obs))\n",
    "resid = L_obs - decay_model_simple(t_obs,medians[0], medians[1], medians[2], medians[3], medians[4])\n",
    "\n",
    "ax2.errorbar(t_obs-plot_start_date,resid/(L_obs_err),sig,fmt='.r',elinewidth=0.5,capsize=0,ms=4.0)\n",
    "\n",
    "ax2.hlines(0,-2,200,linewidth=0.5,color='b')\n",
    "ax2.set_xlabel(f'Time (days) - MJD = {plot_start_date}', fontsize=14)\n",
    "ax2.set_ylabel(r'$\\frac{F_{Obs}-F_{Model}}{\\sigma_F}$', fontsize=16)\n",
    "ax2.set_xlim(0,plot_length)\n",
    "ax2.set_ylim(-5.6,5.6)\n",
    "ax2.minorticks_on()\n",
    "ax2.ticklabel_format(useOffset=False)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.tick_params(axis='both', which='major', length=5)\n",
    "ax2.tick_params(axis='both', which='minor', length=2.5)\n",
    "ax2.tick_params(axis='both', which='both',direction='in',right=True,top=True)\n",
    "\n",
    "fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
